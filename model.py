'''
Created on Jan 5, 2017

@author: praveen.subramanian
'''

import csv
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.utils import shuffle
import numpy as np
import cv2
import time
import json

from keras.models import Sequential
from keras.layers import Input
from keras.layers.core import Dense, Dropout, Activation, Flatten, Lambda
from keras.optimizers import SGD, Adam, RMSprop
from keras.utils import np_utils

from keras.models import model_from_json
from keras.layers.convolutional import Convolution2D
from keras.layers.convolutional import Conv2D
from keras.layers.advanced_activations import ELU
from keras.wrappers.scikit_learn import KerasRegressor
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score

####################################################################
# Reading the training data generated by test drive on the simulator
####################################################################
ifile  = open('/Users/praveen.subramanian/data/carnd/p3/driving_log.csv', "rt")
# ifile  = open('driving_log.csv', "rt") ## This is the file containing the sample data in the project submission page. Could not run this with my local CPU.
reader = csv.reader(ifile)

####################################################################
# Considering only the center image as the input feature as hinted 
# in the project guideline. 
# Considering only the steering angle as the output as required by
# the project guideline.
####################################################################
center_image = []
steering_angle = []

i = 0
for row in reader:
    if i > 0 :
        center_image.append(row[0])
        steering_angle.append(row[3])
    i = i+1

ifile.close()

######################################################
# Printing the no of input features and output labels
######################################################
print("No. of center images = ",len(center_image))
print("No. of steering angles = ",len(steering_angle))

####################################################################
# Assigning values to the input feature dimensions. Since the images
# captured by the center camera in the simulator is 320x160 with RGB
# colors, we take the input shape = (num_images,160,320,3)
####################################################################
num_images = len(center_image)
image_rows = 160
image_columns = 320
image_channels = 3


grey = np.zeros((num_images,image_rows,image_columns,image_channels))

for i in range(num_images):
    im1=cv2.imread(center_image[i])
    grey[i] = im1
    steering_angle[i] = round(float(steering_angle[i]),2)

####################################################################
# Splitting the input feature set into training and validation set
# to check the loss values on the validation set.
####################################################################
X_train, X_val, y_train, y_val = train_test_split(grey,steering_angle , test_size=0.33, random_state=0)

print("Shape of X_train = ",X_train.shape)
print("Shape of y_train = ",len(y_train))
print("Shape of X_val = ",X_val.shape)
print("Shape of y_val = ",len(y_val))

X_train = X_train.astype('float32')
X_val = X_val.astype('float32')

input_shape = (image_rows, image_columns, image_channels)

################################################################################################################################################################################
# This is a function to create a model with 5 convolutional layers, Flatten, Dropout, RELU, Fully Connected Layer, Dropout, RELU followed by 4 Fully Connected layers.
# As learnt from earlier projects, Droput is to avoid overfitting, RELU is to introduce non-linearity in the model. The last Fully Connected layer
# has only one neuron in this model since this is a regression problem given that the output steering angles as continuous values. This is in contrast to the previous projects
# where the output lables were discrete values and hence in the case of German traffic sign database, the number of neurons in the last Fully Connected layer equals the number
# of output classes (i.e. 42 for German Traffic Data Set problem).
#
# References:
# Nvidia paper: http://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf
# Confluence link: https://chatbotslife.com/learning-human-driving-behavior-using-nvidias-neural-network-model-and-image-augmentation-80399360efee#.6ptu4rq9t
#
################################################################################################################################################################################
def creat_model_nvidia():
    
    model = Sequential()

    model.add(Convolution2D(24, 5, 5, input_shape=input_shape, subsample=(2, 2), border_mode="valid", activation='relu'))
    model.add(Convolution2D(36, 5, 5, subsample=(2, 2), border_mode="valid", activation='relu'))
    model.add(Convolution2D(48, 5, 5, subsample=(2, 2), border_mode="valid", activation='relu'))
    model.add(Convolution2D(64, 3, 3, subsample=(1, 1), border_mode="valid", activation='relu'))
    model.add(Convolution2D(64, 3, 3, subsample=(1, 1), border_mode="valid", activation='relu'))
    
    model.add(Flatten())
    #####################################################################################
    # There was not much of difference in output when droput was changed from 0.3 to 0.5
    #####################################################################################
    model.add(Dropout(0.5))
    model.add(Activation('relu'))

    model.add(Dense(1164))
    model.add(Dropout(0.5))
    model.add(Activation('relu'))
    
    model.add(Dense(100))
    model.add(Dense(50))
    model.add(Dense(10))
    model.add(Dense(1))

    return model

model = creat_model_nvidia()
adam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)
model.summary()
#################################################################################
# We use the "mse" function to calculate the loss since its a regression problem.
#################################################################################
model.compile(optimizer=adam, loss='mse')

###########################################################################
# This function follows the Python generator paradigm. Here the function
# does not return the values to be loaded in memory, rather it yields the 
# input features and output values in batches of 100(which is a number I 
# chose to accomodate the memory constraints of my laptop). 
###########################################################################
batch_size = 100
def batchgen(images,output):
    num_images = len(images)
    print("num_images = ",num_images)
    for i in range(0,num_images,batch_size):
        yield (images[i:i + batch_size],output[i:i + batch_size])
 
###########################################################################
# Testing if the batchgen function defined above is working as expected
# before passing it to the keras.model.fit_generator().
###########################################################################     
i = 1  
batch = batchgen(X_train,y_train)
for x,y in batch:
    print("Dimension 1 of image 1",len(x))
    print("************")
    print("Dimension 2 of image 1",len(x[0]))
    print("************")
    print("Dimension 3 of image 1",len(x[0][0]))
    print("************")
    print(y)
    i = i + 1
    if(i > 1):
        break
        
###########################################################################
# For anywhere over epoch size of 5, there is no improvement in the loss.
# First, we use fit_generator method of keras.model given its a regression problem
# and it fits the model on the data generator batch by batch. So it works on low 
# memory resource constrained machines if we tune the batch size accordingly.
# Second, we evaluate the trained model against the validation data set. It returns
# a score of how well it performed on the validation set.
#
# References of fit_generator usage : https://keras.io/models/model/
###########################################################################    
nb_epoch = 5
history = model.fit_generator(batchgen(X_train, y_train), samples_per_epoch = batch_size, nb_epoch = nb_epoch,
                     verbose=1, max_q_size = 10,
                      pickle_safe=False)
score = model.evaluate(X_val, y_val, verbose=0)
print('Test score:', score)


# fix random seed for reproducibility
# seed = 7
# np.random.seed(seed)
# # evaluate model with standardized dataset
# estimator = KerasRegressor(build_fn=model, nb_epoch=100, batch_size=5, verbose=0)
# 
# kfold = KFold(n_splits=10, random_state=seed)
# results = cross_val_score(estimator, X_train, y_train, cv=kfold)
# print("Results: %.2f (%.2f) MSE" % (results.mean(), results.std()))

#################################################
# Saving the trained model as a json file.
# Also, saving the learned weights as h5 file.
#################################################
model_json = model.to_json()
with open('model.json', 'w') as f:
    json.dump(model_json, f)
model.save_weights("model.h5")
